\section{What We Have Done}
We have examined two different generative models of semi-supervised learning including multinomial Naive Bayes and graph-based methods for optimizing the components separation measurement. First and foremost, we have claimed the assumption that the label values are distributed as the dense components and these components are separated in its data representation form. Then what we have wanted to do is improve the methods that have better way to distinguish these components. So then, we have
\begin{itemize}
	\item The multinomial model, constructed with Naive Bayes and many-to-one assumptions, performs its head over the traditional one component per label value. To leverage the use of labeled data, through the experiments, we showed that it is better to consider the initialization with unsupervised methods. It gives us not only the better use of labeled data, but also the more stable with different components sampling than the conventional randomized method.
	
	\item The graph-based methods setup on the minimized of similarity between groups of label components. The problem comes from the overlap region of components when it may produce many optimized solutions. In this situation, the original mincut approach has an inflexible solution when we cannot have a change to intervene in the inference process. Then we changed to utilize the graphical model with the support of influence index. We also extended the model itself to work on a more general case when we employ the loopy belief propagation algorithm. Through our investigations, we held our observation that when giving the advantage to examine and choose the proper graph construction for a specific data, the graphical model shows its outperform to the mincut approach. Moreover, the model has proved itself to be a stable solution in the condition of semi-supervised assumption when we are drowning in the unlabeled data in comparison with our tested supervised models.
\end{itemize}

Ultimately, besides giving more useful modifications in some particular cases, our works have given more evidence on the reasonable of the semi-supervised learning in practice.
 
\section{Shortcomings and Long Term Views}
The limitations go directly from the restrictions of our assumptions. It is also from simplifications and omissions when we modify our models. Nevertheless, it will be the room of our future work. Here we outline them in the objective for both models and subjective for each individual.

\begin{itemize}
	\item First, we go back again to the question of the benefit of unlabeled data. It is argued that the unlabeled data does not always give positive value, and given more unlabeled data may hurt our classifier. In our experiments, we did not cover this problem since it is outside of what we intended to do. Now, in better case, we should find a reasonable amount of unlabeled data or the weighting factor between labeled-unlabeled data that still keeps the classifier working well.
	
 	\item When the components are bent too close to each other, the assumption is to fail in matching the separation boundary. In this case, the multinomial seems to be more stable when it utilizes the statistical model rather than the similarity of data instances as in graph-based methods.
	
	\item From the beginning of chapter \ref{chapter 2}, we have introduced the general form of many-to-one assumption as in (\ref{equal2: general many-to-one assumption}). But when we deployed the multinomial model, we constrained the component to a fixed label value, the predetermined components. However, \citeauthor{NIPS1996_1208} \parencite{NIPS1996_1208} suggested that it is better to release more to a component that is distributed to all the label values. It may be interesting to derive this claim to try with our assignments.
	
	\item The last obvious one is multi-classification. It is natural to extend the multi-classes to multinomial model, but it has no hope with our graph-based methods---we need to change the target function in this case. There are some common schemes for combining binary classifiers like one-versus-one or one-versus-rest. We may refer to them if we want to work on a multi-classification problem.
\end{itemize}

\section{Conclusions}
Finally, we agree with the view point of \citeauthor{Olivier2006} \parencite{Olivier2006}, semi-supervised learning seems to be more practical than theoretical problem. By this time, semi-supervised learning has not been kept in the main stream of machine learning interest. But when we construct a learning system, it frequently has the situations when we need to consult the solution from the semi-supervised models. So that is the reason why we prefer the practical solution in specific cases of semi-supervised than the overall one which competes with the wide range of different methods.